{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction for audio organization\n",
    "Author: Leonardo Pepino \n",
    "\n",
    "In this tutorial dimensionality reduction techniques for audio organization will be explored. You will need the following libraries to run this notebook:\n",
    "\n",
    "- Numpy\n",
    "- Librosa\n",
    "- Sounddevice\n",
    "- Scikit-learn\n",
    "- Keras\n",
    "- Tensorflow\n",
    "- Matplotlib\n",
    "- Umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Invalid GUI request 'widget', valid ones are:dict_keys(['qt', 'ipympl', 'tk', 'nbagg', 'osx', None, 'qt4', 'notebook', 'wx', 'qt5', 'gtk3', 'inline', 'gtk'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras.layers import Input,Dense,Conv2D,MaxPooling2D,UpSampling2D, Reshape, Flatten, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as k\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need many audio files. In order to make the task easier, audio samples should be short. In this notebook, I will use the NSynth Dataset test set which provides 4096 samples of notes being played by different musical instruments. Samples are 4 seconds long. You can download the samples at: https://magenta.tensorflow.org/datasets/nsynth. Once you download them, place audio files in a folder called \"samples\" within the notebook directory.\n",
    "\n",
    "Now that the dataset was downloaded, let's meet the data:\n",
    "- Mono/Stereo?\n",
    "- Sampling rate?\n",
    "- All files same length?\n",
    "- Listen to some samples -> Do we need to take into account temporal evolution? -> Stationary? -> Silences? -> Noise? -> Percussive or tonal? -> ...\n",
    "\n",
    "If you don't want to wait a long time, do not run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel numbers in dataset:\n",
      "[1.]\n",
      "Sample lengths in dataset:\n",
      "[88200.]\n",
      "Sampling rates in dataset:\n",
      "[22050.]\n"
     ]
    }
   ],
   "source": [
    "path = \".//samples\"\n",
    "filelist = os.listdir(path) #Get all audio files in a list\n",
    "nchannels = np.zeros((len(filelist),))\n",
    "lengths = np.zeros((len(filelist),))\n",
    "srs = np.zeros((len(filelist),))\n",
    "for i, file in enumerate(filelist): #Open each file and store the number of channels, length and sampling rates.\n",
    "    [audio,fs] = librosa.load(path + '//' + file)\n",
    "    tensorshape = np.shape(audio)\n",
    "    if (len(tensorshape) == 1):\n",
    "        nchannels[i] = 1\n",
    "    else:\n",
    "        nchannels[i] = tensorshape[1]\n",
    "    lengths[i] = tensorshape[0]\n",
    "    srs[i] = fs\n",
    "nchannels = np.unique(nchannels)\n",
    "lengths = np.unique(lengths)\n",
    "srs = np.unique(srs)\n",
    "\n",
    "print('Channel numbers in dataset:')\n",
    "print(nchannels)\n",
    "print('Sample lengths in dataset:')\n",
    "print(lengths)\n",
    "print('Sampling rates in dataset:')\n",
    "print(srs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset is pretty nice because all samples have the same duration, sampling rate and number of channels. If we listen to them we will notice that most of the audio samples spectrum does not change along time (they are stationary) and there are no silences. \n",
    "So, each audio sample is a vector of 88200 elements. The goal of this tutorial will be to \"summarize\" those 88200 elements in just 2 numbers, which will be represented as a 2D point. So, dimensionality reduction has to be performed (from 88200 to 2 dimensions) and the purpose is visualization, because it is hard to see in 88200 dimensions. We are limited to visualizing things in 3 dimensions or less (although we could use color as a fourth dimension, and time as a fifth if we animate the plots, but it would be hard to interpretate).\n",
    "\n",
    "Some possible approaches to perform dimensionality reduction in audio are:\n",
    "\n",
    "- Start from raw data -> could be directly the samples or maybe a spectrogram -> Most traditional methods can't deal with very high dimensionality -> Use deep learning -> Autoencoders -> Features are obtained -> could be 2 so that we do not need further dimensionality reduction, or a larger number, but we would need to reduce dimensionality again with another technique.\n",
    "- Manually extract some features -> Mel frequency cepstral coefficients, Loudness, Roughness, Spectral Rolloff, ... -> These are hand-crafted features and will let us reduce a bit of dimensionality -> Then we can apply methods like PCA, tSNE, UMap,...   \n",
    "\n",
    "In this notebook I used MFCC instead of raw data... raw data is for a next tutorial. Anyway, we will train an autoencoder ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first step is to get the MFCC. LibROSA library has a function that does it for us. I chose the frames to be 2048 samples long with an overlap factor of 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organisedataset(filelist):\n",
    "    featurevector = []\n",
    "    filevector=[]\n",
    "    nfft = 2048 #How many samples for each analysis window\n",
    "    hop_size = 1024 #This gives us 50% of overlap\n",
    "    \n",
    "    for audiofile in filelist:\n",
    "        [audio,fs] = librosa.load(path + \"\\\\\" + audiofile)\n",
    "        mfcc = librosa.feature.mfcc(y=audio,sr=fs,n_fft = nfft,hop_length = hop_size,n_mfcc=32) #We will get 32 MFCCs for each analysis window  \n",
    "        featurevector.append(mfcc)\n",
    "        filevector.append(audiofile) #Also store the filenames for later.\n",
    "    \n",
    "    return [featurevector,filevector]\n",
    "\n",
    "path = \".//samples\"\n",
    "filelist = os.listdir(path)\n",
    "[xvector,metavector] = organisedataset(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a list with 2 elements. The first is a numpy tensor with the features for all audio samples. The second is a list of the filenames which we will need for the interactive exploration.\n",
    "\n",
    "If running the cell above took a long time, it will be a good idea to save the results to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('nsynthmfcc.npy',[xvector,metavector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the tensor with features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our tensor is:\n",
      "(4096, 32, 87)\n",
      "Minimum value: -954.6725670966737\n",
      "Maximum value: 336.847264015782\n",
      "Mean value: -10.819791192414431\n",
      "Standard deviation: 83.45644645399183\n"
     ]
    }
   ],
   "source": [
    "print('The shape of our tensor is:')\n",
    "print(np.shape(xvector))\n",
    "print('Minimum value: ' + str(np.min(xvector)))\n",
    "print('Maximum value: ' + str(np.max(xvector)))\n",
    "print('Mean value: ' + str(np.mean(xvector)))\n",
    "print('Standard deviation: ' + str(np.std(xvector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the features take a large range, so a possible strategy is to turn the values into z-scores, so that the mean is 0 and the standard deviation 1.\n",
    "But first, I don't like the number 87, so we will use just the first 32 frames, but... is it right? As I said at the beginning, samples in NSynth dataset do not change a lot over time, so I think that the first 32 frames will be representative of the 87 and will let us work in an easier way with autoencoders (no paddings and that stuff...), but remember: if You are using another dataset maybe you will need all the frames, or maybe just to take some global descriptors such as the mean, std, kurtosis, skewness... of each MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our tensor is:\n",
      "(4096, 32, 32)\n",
      "Minimum value: -5.444073214861761\n",
      "Maximum value: 7.6798043887103145\n",
      "Mean value: 3.1712913545201005e-17\n",
      "Standard deviation: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((len(xvector),32,32))\n",
    "for k, el in enumerate(xvector):\n",
    "    x[k,:,:] = el[:,:32]\n",
    "    \n",
    "xnorm = (x - np.mean(x,axis=0))/np.std(x,axis=0)\n",
    "\n",
    "print('The shape of our tensor is:')\n",
    "print(np.shape(xnorm))\n",
    "print('Minimum value: ' + str(np.min(xnorm)))\n",
    "print('Maximum value: ' + str(np.max(xnorm)))\n",
    "print('Mean value: ' + str(np.mean(xnorm)))\n",
    "print('Standard deviation: ' + str(np.std(xnorm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is a little better, but I don't like the range of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our tensor is:\n",
      "(4096, 32, 32)\n",
      "Minimum value: -0.7088817552260592\n",
      "Maximum value: 1.0\n",
      "Mean value: 2.0599841277224584e-18\n",
      "Standard deviation: 0.1302116498527031\n"
     ]
    }
   ],
   "source": [
    "xnorm = xnorm/np.max(np.abs(xnorm))\n",
    "\n",
    "print('The shape of our tensor is:')\n",
    "print(np.shape(xnorm))\n",
    "print('Minimum value: ' + str(np.min(xnorm)))\n",
    "print('Maximum value: ' + str(np.max(xnorm)))\n",
    "print('Mean value: ' + str(np.mean(xnorm)))\n",
    "print('Standard deviation: ' + str(np.std(xnorm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is in a better range, although std was modified also, but let's see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a magic function will be defined which will allow us to make a 2D scatter plot and when clicking over a point, the associated audio file will be played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onpick(event):\n",
    "    ind = event.ind #Get the index of the clicked point\n",
    "    if len(ind)>1: #If there were many points near the click point, choose the closest one\n",
    "        point = event.artist\n",
    "        datax = point.get_xdata()\n",
    "        datay = point.get_ydata()\n",
    "        datax,datay = [datax[i] for i in ind],[datay[i] for i in ind]\n",
    "        msx, msy = event.mouseevent.xdata, event.mouseevent.ydata\n",
    "        dist = np.sqrt((np.array(datax)-msx)**2 + (np.array(datay)-msy)**2)\n",
    "        ind = ind[np.argmin(dist)]\n",
    "    else:\n",
    "        ind = ind[0]\n",
    "    [audio,fs] = librosa.load(path + \"//\" + metavector[ind]) #Load the audio file associated with the clicked point\n",
    "    sd.play(audio,fs) #Play it\n",
    "    \n",
    "    \n",
    "def interactiveplot(dimreduced,title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    line = ax.plot(dimreduced[:,0],dimreduced[:,1],'o',markersize=2,picker = 5)  \n",
    "    fig.canvas.mpl_connect('pick_event',onpick)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see what happens if we apply Principal Component Analysis (PCA) directly on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnorm1D = np.reshape(xnorm,(4096,1024)) #The first axis is for each sample and the second for each feature.\n",
    "pcasolver = PCA(n_components = 2) #We could set n_components = 3 and visualize it in 3D. Try it!\n",
    "dimreducedPCA = pcasolver.fit_transform(xnorm1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well a first thing to notice is that PCA is really fast even with 1024 dimensions! and you know, time is gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f907f014624d4f20a5d032e2ea3d0028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactiveplot(dimreducedPCA,'A map of sounds: PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this map there are no clear clusters, but if you explore the sounds, similar sounds are close and that is good. So PCA in spite of being a linear technique and very fast, gave us decent results.\n",
    "\n",
    "Before building the autoencoder, let's try using UMap which can also deal with large dimensionality. UMap is not included in sklearn, so we need to install UMap library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1a1c0eca8d4523960cb381d1506dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "umapsolver = umap.UMAP()\n",
    "dimreducedUMAP = umapsolver.fit_transform(xnorm1D)\n",
    "\n",
    "interactiveplot(dimreducedUMAP, 'A map of sounds - UMAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was fast too! and results are a lot better. There are clear clusters which share similar features such as: pitch, temporal evolution (for example vibratos or glissandos), timbre, etc...\n",
    "\n",
    "One thing to notice is that if you run the cell above multiple times, results will differ...\n",
    "\n",
    "UMap is great!\n",
    "\n",
    "But now it is the turn of the neural networks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 16)        1040      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 32)          8224      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 16)          2064      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 16)        1040      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 1)         65        \n",
      "=================================================================\n",
      "Total params: 39,521\n",
      "Trainable params: 39,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "4096/4096 [==============================] - 9s 2ms/step - loss: 0.0152\n",
      "Epoch 2/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0130\n",
      "Epoch 3/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0112\n",
      "Epoch 4/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0101\n",
      "Epoch 5/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0093\n",
      "Epoch 6/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0086\n",
      "Epoch 7/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0080\n",
      "Epoch 8/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0076\n",
      "Epoch 9/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0071\n",
      "Epoch 10/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0066\n",
      "Epoch 11/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0064\n",
      "Epoch 12/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0061\n",
      "Epoch 13/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0058\n",
      "Epoch 14/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0057\n",
      "Epoch 15/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0055\n",
      "Epoch 16/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0054\n",
      "Epoch 17/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0052\n",
      "Epoch 18/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0051\n",
      "Epoch 19/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0049\n",
      "Epoch 20/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0049\n",
      "Epoch 21/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0048\n",
      "Epoch 22/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0047\n",
      "Epoch 23/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0046\n",
      "Epoch 24/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0045\n",
      "Epoch 25/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0044\n",
      "Epoch 26/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0044\n",
      "Epoch 27/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0043\n",
      "Epoch 28/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0042\n",
      "Epoch 29/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0042\n",
      "Epoch 30/50\n",
      "4096/4096 [==============================] - 7s 2ms/step - loss: 0.0041\n",
      "Epoch 31/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0041\n",
      "Epoch 32/50\n",
      "4096/4096 [==============================] - 9s 2ms/step - loss: 0.0040\n",
      "Epoch 33/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0040\n",
      "Epoch 34/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0040\n",
      "Epoch 35/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0039\n",
      "Epoch 36/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0039\n",
      "Epoch 37/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0038\n",
      "Epoch 38/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0038\n",
      "Epoch 39/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0038\n",
      "Epoch 40/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0038\n",
      "Epoch 41/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0037\n",
      "Epoch 42/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0037\n",
      "Epoch 43/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0036\n",
      "Epoch 44/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0036\n",
      "Epoch 45/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0036\n",
      "Epoch 46/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0035\n",
      "Epoch 47/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0035\n",
      "Epoch 48/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0035\n",
      "Epoch 49/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0035\n",
      "Epoch 50/50\n",
      "4096/4096 [==============================] - 8s 2ms/step - loss: 0.0034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2282c5a8908>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnormNN = np.reshape(xnorm,(len(xvector),32,32,1)) #We give the array a shape to work with Keras\n",
    "\n",
    "input_feature = Input(shape=(32,32,1)) #This is the input layer\n",
    "\n",
    "encoder = Conv2D(16,(2,2),padding='same')(input_feature) #The encoder applies convolution layers followed by max-pooling layers multiple times\n",
    "encoder = MaxPooling2D((2,2))(encoder)\n",
    "encoder = Conv2D(16,(2,2),padding='same')(encoder)\n",
    "encoder = MaxPooling2D((2,2))(encoder)\n",
    "encoder = Conv2D(32,(2,2),padding='same')(encoder)\n",
    "encoder = MaxPooling2D((2,2))(encoder)\n",
    "encoder = Conv2D(64,(2,2),padding='same')(encoder)\n",
    "encoder = MaxPooling2D((2,2))(encoder) #Here we have (2,2,64) tensors -> 256 elements\n",
    "encoder = Flatten()(encoder) #We make the outputs flat so that they can be fed to a fully connected layer\n",
    "encoded = Dense(32,activation = \"relu\")(encoder) #This is the latent space\n",
    "decoder = Dense(256,activation = \"relu\")(encoded) #Here the decoder starts, it is symmetrical with respect to the encoder\n",
    "decoder = Reshape((2,2,64))(decoder) \n",
    "decoder = UpSampling2D((2,2))(decoder)\n",
    "decoder = Conv2D(32,(2,2),padding = 'same')(decoder)\n",
    "decoder = UpSampling2D((2,2))(decoder)\n",
    "decoder = Conv2D(16,(2,2),padding = 'same')(decoder)\n",
    "decoder = UpSampling2D((2,2))(decoder)\n",
    "decoder = Conv2D(16,(2,2),padding = 'same')(decoder)\n",
    "decoder = UpSampling2D((2,2))(decoder)\n",
    "decoder = Conv2D(1,(2,2),padding = 'same')(decoder)\n",
    "autoencoder = Model(input_feature,decoder) #The input and output nodes of the model are specified\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer = 'adam',loss = 'mse') #Adam usually gives me good results\n",
    "autoencoder.fit(xnormNN,xnormNN,epochs = 50,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took some time so we will save the trained model in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('ae.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if the autoencoder can reconstruct the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fee7c5b80ce4ef49d60d1981e734fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xpred = autoencoder.predict(xnormNN)\n",
    "fig, axes = plt.subplots(4,4)\n",
    "axes[0,0].set_title('Original')\n",
    "axes[0,1].set_title('Reconstructed')\n",
    "axes[0,2].set_title('Original')\n",
    "axes[0,3].set_title('Reconstructed')\n",
    "for i, j in enumerate(np.random.randint(100,4092,size=(4,))):\n",
    "    axes[i,0].imshow(xnormNN[j,:,:,0]) \n",
    "    axes[i,1].imshow(xpred[j,:,:,0])\n",
    "    axes[i,2].imshow(xnormNN[j-100,:,:,0]) \n",
    "    axes[i,3].imshow(xpred[j-100,:,:,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstructions are not so good as those you would see if using an autoencoder with MNIST, but anyway they reconstruct the overall \"shape\".\n",
    "\n",
    "Now that our samples are represented in a lower dimensional space (32), the popular technique tSNE can be applied, but... what happens if we use an autoencoder with a latent space of 2 neurons for visualization. Let's try it!\n",
    "\n",
    "First, let's get the latent space neurons output for all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 32)\n"
     ]
    }
   ],
   "source": [
    "encoder1 = Model(input_feature,encoded) #This model has the latent space as output\n",
    "latentspace1 = encoder1.predict(xnormNN)\n",
    "print(np.shape(latentspace1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!, now it is time to build a new small autoencoder which will take these 32 dimensional inputs and reconstruct them in its output from a 2 dimensional latent space. This autoencoder will be a variational one (VAE) and the code is based on keras tutorials. The idea is that the latent space neurons will output the mean and standard deviation of a normal distribution instead of values, and then we will sample from the resulting distribution and decode it. This generates a better-formed latent space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 16)           528         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 2)            34          dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 2)            34          dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 2)            0           dense_35[0][0]                   \n",
      "                                                                 dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 16)           48          lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 32)           544         dense_37[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,188\n",
      "Trainable params: 1,188\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "4096/4096 [==============================] - 1s 169us/step - loss: 0.3080\n",
      "Epoch 2/50\n",
      "4096/4096 [==============================] - 0s 24us/step - loss: 0.2693\n",
      "Epoch 3/50\n",
      "4096/4096 [==============================] - 0s 22us/step - loss: 0.2325\n",
      "Epoch 4/50\n",
      "4096/4096 [==============================] - 0s 31us/step - loss: 0.2131\n",
      "Epoch 5/50\n",
      "4096/4096 [==============================] - 0s 33us/step - loss: 0.2074\n",
      "Epoch 6/50\n",
      "4096/4096 [==============================] - 0s 30us/step - loss: 0.2036\n",
      "Epoch 7/50\n",
      "4096/4096 [==============================] - 0s 32us/step - loss: 0.2007\n",
      "Epoch 8/50\n",
      "4096/4096 [==============================] - 0s 29us/step - loss: 0.1984\n",
      "Epoch 9/50\n",
      "4096/4096 [==============================] - 0s 32us/step - loss: 0.1973\n",
      "Epoch 10/50\n",
      "4096/4096 [==============================] - 0s 32us/step - loss: 0.1966\n",
      "Epoch 11/50\n",
      "4096/4096 [==============================] - 0s 26us/step - loss: 0.1962\n",
      "Epoch 12/50\n",
      "4096/4096 [==============================] - 0s 35us/step - loss: 0.1958\n",
      "Epoch 13/50\n",
      "4096/4096 [==============================] - 0s 40us/step - loss: 0.1947\n",
      "Epoch 14/50\n",
      "4096/4096 [==============================] - 0s 25us/step - loss: 0.1947\n",
      "Epoch 15/50\n",
      "4096/4096 [==============================] - 0s 29us/step - loss: 0.1946\n",
      "Epoch 16/50\n",
      "4096/4096 [==============================] - 0s 24us/step - loss: 0.1944\n",
      "Epoch 17/50\n",
      "4096/4096 [==============================] - 0s 32us/step - loss: 0.1938\n",
      "Epoch 18/50\n",
      "4096/4096 [==============================] - 0s 26us/step - loss: 0.1935\n",
      "Epoch 19/50\n",
      "4096/4096 [==============================] - 0s 21us/step - loss: 0.1935\n",
      "Epoch 20/50\n",
      "4096/4096 [==============================] - 0s 32us/step - loss: 0.1931\n",
      "Epoch 21/50\n",
      "4096/4096 [==============================] - 0s 27us/step - loss: 0.1928\n",
      "Epoch 22/50\n",
      "4096/4096 [==============================] - 0s 23us/step - loss: 0.1923\n",
      "Epoch 23/50\n",
      "4096/4096 [==============================] - 0s 26us/step - loss: 0.1926\n",
      "Epoch 24/50\n",
      "4096/4096 [==============================] - 0s 35us/step - loss: 0.1920\n",
      "Epoch 25/50\n",
      "4096/4096 [==============================] - 0s 39us/step - loss: 0.1922\n",
      "Epoch 26/50\n",
      "4096/4096 [==============================] - 0s 22us/step - loss: 0.1919\n",
      "Epoch 27/50\n",
      "4096/4096 [==============================] - 0s 28us/step - loss: 0.1918\n",
      "Epoch 28/50\n",
      "4096/4096 [==============================] - 0s 21us/step - loss: 0.1918\n",
      "Epoch 29/50\n",
      "4096/4096 [==============================] - 0s 34us/step - loss: 0.1918\n",
      "Epoch 30/50\n",
      "4096/4096 [==============================] - 0s 24us/step - loss: 0.1917\n",
      "Epoch 31/50\n",
      "4096/4096 [==============================] - 0s 22us/step - loss: 0.1919\n",
      "Epoch 32/50\n",
      "4096/4096 [==============================] - 0s 24us/step - loss: 0.1914\n",
      "Epoch 33/50\n",
      "4096/4096 [==============================] - 0s 22us/step - loss: 0.1916\n",
      "Epoch 34/50\n",
      "4096/4096 [==============================] - 0s 30us/step - loss: 0.1915\n",
      "Epoch 35/50\n",
      "4096/4096 [==============================] - 0s 28us/step - loss: 0.1912\n",
      "Epoch 36/50\n",
      "4096/4096 [==============================] - 0s 23us/step - loss: 0.1915\n",
      "Epoch 37/50\n",
      "4096/4096 [==============================] - 0s 31us/step - loss: 0.1912\n",
      "Epoch 38/50\n",
      "4096/4096 [==============================] - 0s 18us/step - loss: 0.1910\n",
      "Epoch 39/50\n",
      "4096/4096 [==============================] - 0s 24us/step - loss: 0.1910\n",
      "Epoch 40/50\n",
      "4096/4096 [==============================] - 0s 26us/step - loss: 0.1910\n",
      "Epoch 41/50\n",
      "4096/4096 [==============================] - 0s 30us/step - loss: 0.1908\n",
      "Epoch 42/50\n",
      "4096/4096 [==============================] - 0s 24us/step - loss: 0.1909\n",
      "Epoch 43/50\n",
      "4096/4096 [==============================] - 0s 20us/step - loss: 0.1907\n",
      "Epoch 44/50\n",
      "4096/4096 [==============================] - 0s 21us/step - loss: 0.1908\n",
      "Epoch 45/50\n",
      "4096/4096 [==============================] - 0s 23us/step - loss: 0.1907\n",
      "Epoch 46/50\n",
      "4096/4096 [==============================] - 0s 17us/step - loss: 0.1909\n",
      "Epoch 47/50\n",
      "4096/4096 [==============================] - 0s 15us/step - loss: 0.1905\n",
      "Epoch 48/50\n",
      "4096/4096 [==============================] - 0s 18us/step - loss: 0.1904\n",
      "Epoch 49/50\n",
      "4096/4096 [==============================] - 0s 35us/step - loss: 0.1910\n",
      "Epoch 50/50\n",
      "4096/4096 [==============================] - 0s 31us/step - loss: 0.1902\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as k\n",
    "\n",
    "def sampling(args):\n",
    "    encodedmean, encodedlogsigma = args\n",
    "    epsilon = k.random_normal(shape=(128,2),mean = 0, stddev=0.1)\n",
    "    \n",
    "    return encodedmean + k.exp(encodedlogsigma)*epsilon\n",
    "\n",
    "input_latent = Input(shape=(32,))\n",
    "encoder = Dense(16,activation = \"relu\")(input_latent)\n",
    "encodedmean = Dense(2)(encoder)\n",
    "encodedlogsigma = Dense(2)(encoder)\n",
    "z = Lambda(sampling,output_shape=(2,))([encodedmean,encodedlogsigma])\n",
    "decoder = Dense(16,activation = \"relu\")(z)\n",
    "decoder = Dense(32,activation = \"relu\")(decoder)\n",
    "\n",
    "def vaeloss(x,x_decoded):\n",
    "    x_loss = k.sqrt(k.mean(k.square(x-x_decoded)))\n",
    "    kl_loss = -0.5*k.mean(1+encodedlogsigma-k.square(encodedmean)-k.exp(encodedlogsigma),axis = -1)\n",
    "    return x_loss + kl_loss\n",
    "    \n",
    "autoencoder2 = Model(input_latent,decoder)\n",
    "autoencoder2.summary()\n",
    "autoencoder2.compile(optimizer = 'adam',loss = vaeloss)\n",
    "autoencoder2.fit(latentspace1,latentspace1,epochs = 50,batch_size=128)\n",
    "\n",
    "encoder2 = Model(input_latent,encodedmean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8f0b0fbf794c8aa2973d8b2aa76087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dimreducedAE = encoder2.predict(latentspace1)\n",
    "interactiveplot(dimreducedAE, 'A map of sounds - AE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is not so good. Let's keep the 32 dimensional embeddings and use PCA, tSNE and UMap and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e144f73c9bfb4d62af595ed755d3c266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcasolver = PCA(n_components = 2)\n",
    "dimreducedAEPCA = pcasolver.fit_transform(latentspace1)\n",
    "interactiveplot(dimreducedAEPCA, 'A map of sounds - AE + PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA again was fast but did not help with the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6dbed166d94dca8a447ab7d9da97f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsnesolver = TSNE(n_components = 2)\n",
    "dimreducedAETSNE = tsnesolver.fit_transform(latentspace1)\n",
    "interactiveplot(dimreducedAETSNE, 'A map of sounds - AE + TSNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took much more time but results are really nice. There are many clusters associated with pitch, timbre and envelope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b07ab28e344edbada9e2668470bfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "umapsolver = umap.UMAP()\n",
    "dimreducedUMAP = umapsolver.fit_transform(latentspace1)\n",
    "\n",
    "interactiveplot(dimreducedUMAP, 'A map of sounds - AE + UMAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems cluttered. Let's play a bit with UMap parameters. More neighbors tend to preserve more the global structure than the local structure of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0613c6b981482b8f7b48658a49710c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "umapsolver2 = umap.UMAP(n_neighbors = 50,min_dist = 0.5)\n",
    "dimreducedUMAP = umapsolver.fit_transform(latentspace1)\n",
    "\n",
    "interactiveplot(dimreducedUMAP, 'A map of sounds - AE + UMAP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's play a bit with tSNE also. Changing perplexity can lead to very different visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a54b551af44f66a2b867a3139dbc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FigureCanvasNbAgg</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsnesolver = TSNE(n_components = 2, perplexity = 50, learning_rate = 500, n_iter = 1000)\n",
    "dimreducedaetsne2 = tsnesolver.fit_transform(latentspace1)\n",
    "interactiveplot(dimreducedaetsne2, 'A map of sounds - AE + TSNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I think the winner was Autoencoder + tSNE, but that is subjective so the important thing is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we learnt:\n",
    "\n",
    "- Preparing audio data and extracting features.\n",
    "- Using sklearn for dimensionality reduction and umap.\n",
    "- UMap is a great tool for visualization as it can deal with very high dimensional data.\n",
    "- Autoencoders alone maybe are not so good for data visualization, but can deal with very high dimensional data and extract features automatically. We learnt to build Convolutional AE and Variational AE.\n",
    "- tSNE gives good visualizations but can't deal with very high dimensional data.\n",
    "- A good approach is extracting features with an autoencoder and then applying tSNE.\n",
    "\n",
    "What to improve:\n",
    "\n",
    "- Try with other features than MFCC. There are a loooot...\n",
    "- Deal with raw data and exploit deep learning models (Wavenet Autoencoders? mmm give me many GPUs, Convolutional Autoencoders applied to a time-frequency representation like STFT, CQT, Wavelets..., there are a lot of options but they will give your machine a lot of hard work.)\n",
    "- Explore other audio datasets, maybe the whole NSynth but with just one pitch so the clusters are not pitch-related and instead are timbre/envelope related. Animal sounds? Drums sounds? Songs? Speakers? Do you have a really large foley sounds library and can't find the right sound? or maybe you are a sound designer with a lot of samples difficult to organise... There are a lot of options to keep playing with audio exploration/visualization, just choose one and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Any suggestions, feedback, corrections, send me an email to leonardodpepino@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
